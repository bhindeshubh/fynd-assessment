{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06c17596",
   "metadata": {},
   "source": [
    "# TASK 1: Yelp Rating Prediction via Prompting (OpenRouter Version)\n",
    "\n",
    "This notebook uses OpenRouter's free LLMs.\n",
    "You can experiment with different free models available on OpenRouter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cac8321",
   "metadata": {},
   "source": [
    "## 1. IMPORTS AND SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647d999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.metrics import confusion_matrix, mean_absolute_error, accuracy_score\n",
    "import requests\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370342a",
   "metadata": {},
   "source": [
    "## 2. OPENROUTER API CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb49974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# OpenRouter API endpoint\n",
    "OPENROUTER_API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "# You can experiment with different models!\n",
    "# AVAILABLE_FREE_MODELS = {\n",
    "#     \"mistral-7b\": \"mistralai/mistral-7b-instruct:free\",\n",
    "#     \"openchat-7b\": \"openchat/openchat-7b:free\",\n",
    "#     \"mythomax-13b\": \"gryphe/mythomax-l2-13b:free\",\n",
    "#     \"toppy-7b\": \"undi95/toppy-m-7b:free\",\n",
    "#     \"cinematika-7b\": \"openrouter/cinematika-7b:free\",\n",
    "#     \"gemma-7b\": \"google/gemma-7b-it:free\",\n",
    "#     \"mythomist-7b\": \"gryphe/mythomist-7b:free\",\n",
    "# }\n",
    "\n",
    "# Choose which model to use\n",
    "SELECTED_MODEL = \"mistralai/mistral-7b-instruct:free\"  # Change this to experiment!\n",
    "\n",
    "print(f\"‚úÖ Using OpenRouter model: {SELECTED_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f8e1a",
   "metadata": {},
   "source": [
    "## 3. OPENROUTER API HELPER FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b4f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openrouter_api(prompt: str, temperature: float = 0.1, max_tokens: int = 300) -> str:\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": SELECTED_MODEL,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": temperature,\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            OPENROUTER_API_URL,\n",
    "            headers=headers,\n",
    "            json=data,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        # Extract the response text\n",
    "        return result['choices'][0]['message']['content']\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        raise Exception(f\"API request failed: {str(e)}\")\n",
    "    except KeyError as e:\n",
    "        raise Exception(f\"Unexpected API response format: {str(e)}\")\n",
    "\n",
    "\n",
    "def test_api_connection():\n",
    "    \n",
    "    print(\"\\nüîç Testing OpenRouter API connection...\")\n",
    "    \n",
    "    try:\n",
    "        response = call_openrouter_api(\n",
    "            \"Say 'Hello' in JSON format: {\\\"message\\\": \\\"your message\\\"}\",\n",
    "            temperature=0.0,\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(f\"‚úÖ API connection successful!\")\n",
    "        print(f\"Test response: {response}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå API connection failed: {str(e)}\")\n",
    "        print(\"\\nüí° Tips:\")\n",
    "        print(\"1. Get free API key from: https://openrouter.ai/keys\")\n",
    "        print(\"2. Update OPENROUTER_API_KEY in this notebook\")\n",
    "        print(\"3. Make sure you have credits (free tier included)\")\n",
    "        return False\n",
    "\n",
    "# Test the connection\n",
    "if not test_api_connection():\n",
    "    print(\"\\n‚ö†Ô∏è Please fix API configuration before continuing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d79344",
   "metadata": {},
   "source": [
    "## 4. DATA LOADING AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fec51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_sample_data(filepath: str, sample_size: int = 250) -> pd.DataFrame:\n",
    "    \n",
    "    print(f\"\\nüìÇ Loading data from {filepath}...\")\n",
    "    df = pd.read_csv(filepath)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nRating distribution:\")\n",
    "    print(df['stars'].value_counts().sort_index())\n",
    "    \n",
    "    # Create balanced sample (50 reviews per star rating)\n",
    "    per_rating = sample_size // 5\n",
    "    sampled_dfs = []\n",
    "    \n",
    "    for rating in range(1, 6):\n",
    "        rating_df = df[df['stars'] == rating].sample(\n",
    "            n=min(per_rating, len(df[df['stars'] == rating])), \n",
    "            random_state=42\n",
    "        )\n",
    "        sampled_dfs.append(rating_df)\n",
    "    \n",
    "    sample_df = pd.concat(sampled_dfs, ignore_index=True)\n",
    "    sample_df = sample_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Sampled {len(sample_df)} reviews\")\n",
    "    print(f\"Sample rating distribution:\")\n",
    "    print(sample_df['stars'].value_counts().sort_index())\n",
    "    \n",
    "    return sample_df\n",
    "\n",
    "# Load the data (update path to your file)\n",
    "df = load_and_sample_data('data/yelp.csv', sample_size=250)\n",
    "\n",
    "# Display sample reviews\n",
    "print(\"\\nüìù Sample reviews:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\nRating: {df.iloc[i]['stars']} stars\")\n",
    "    print(f\"Review: {df.iloc[i]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf34c615",
   "metadata": {},
   "source": [
    "## 5. PROMPTING APPROACHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261df708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptApproach:\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "    \n",
    "    def create_prompt(self, review_text: str) -> str:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def predict(self, review_text: str, temperature: float = 0.1) -> Dict:\n",
    "        \n",
    "        try:\n",
    "            # Create the prompt\n",
    "            prompt = self.create_prompt(review_text)\n",
    "            \n",
    "            # Verify prompt is a string\n",
    "            if not isinstance(prompt, str):\n",
    "                raise TypeError(f\"Prompt must be a string, got {type(prompt)}\")\n",
    "            \n",
    "            # Call OpenRouter API\n",
    "            response_text = call_openrouter_api(prompt, temperature=temperature, max_tokens=300)\n",
    "            response_text = response_text.strip()\n",
    "            print(response_text)\n",
    "            \n",
    "            # Try to find JSON in the response\n",
    "            if '```json' in response_text:\n",
    "                json_start = response_text.find('```json') + 7\n",
    "                json_end = response_text.find('```', json_start)\n",
    "                response_text = response_text[json_start:json_end].strip()\n",
    "            elif '```' in response_text:\n",
    "                json_start = response_text.find('```') + 3\n",
    "                json_end = response_text.find('```', json_start)\n",
    "                response_text = response_text[json_start:json_end].strip()\n",
    "            \n",
    "            # Parse JSON\n",
    "            result = json.loads(response_text)\n",
    "            \n",
    "            # Validate the result\n",
    "            if 'predicted_stars' not in result:\n",
    "                return {\"predicted_stars\": None, \"explanation\": \"Invalid JSON structure\", \"valid_json\": False}\n",
    "            \n",
    "            # Ensure rating is between 1-5\n",
    "            predicted_stars = result['predicted_stars']\n",
    "            if not isinstance(predicted_stars, (int, float)):\n",
    "                return {\"predicted_stars\": None, \"explanation\": \"predicted_stars must be a number\", \"valid_json\": False}\n",
    "            \n",
    "            predicted_stars = int(predicted_stars)\n",
    "            if not (1 <= predicted_stars <= 5):\n",
    "                predicted_stars = max(1, min(5, predicted_stars))\n",
    "            \n",
    "            return {\n",
    "                \"predicted_stars\": predicted_stars,\n",
    "                \"explanation\": result.get('explanation', 'No explanation provided'),\n",
    "                \"valid_json\": True\n",
    "            }\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            return {\"predicted_stars\": None, \"explanation\": f\"JSON parsing failed: {str(e)}\", \"valid_json\": False}\n",
    "        except Exception as e:\n",
    "            return {\"predicted_stars\": None, \"explanation\": f\"Error: {str(e)}\", \"valid_json\": False}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e64dac",
   "metadata": {},
   "source": [
    "### Approach 1 : Zero-Shot Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a00f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroShotApproach(PromptApproach):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name = \"Zero-Shot Prompt\",\n",
    "            description = \"Simple, direct instruction with no examples\"\n",
    "        )\n",
    "    \n",
    "    def create_prompt(self, review_text: str) -> str:\n",
    "\n",
    "        zero_shot_prompt = \"\"\"\n",
    "        You are a rating prediction system for Yelp reviews. Analyze the following review and predict the star rating (1-5 stars).\n",
    "\n",
    "        Rating Guidelines:\n",
    "        - 5 stars: Excellent, highly positive\n",
    "        - 4 stars: Good, mostly positive with minor issues\n",
    "        - 3 stars: Average, mixed feelings\n",
    "        - 2 stars: Poor, mostly negative with some positives\n",
    "        - 1 star: Terrible, extremely negative\n",
    "\n",
    "        Review: \"{review_text}\"\n",
    "\n",
    "        Return your response in this EXACT JSON format (no markdown, no extra text):\n",
    "        {{\n",
    "          \"predicted_stars\": <number between 1-5>,\n",
    "          \"explanation\": \"<brief reasoning in 1-2 sentences>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "\n",
    "        return zero_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14754b3",
   "metadata": {},
   "source": [
    "### Approach 2 : Few Shot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4a4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FewShotApproach(PromptApproach):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name = \"Few Shot With Examples\",\n",
    "            description = \"Provides 5 examples (one for each rating) to guide the model\"\n",
    "        )\n",
    "    \n",
    "    def create_prompt(self, review_text: str) -> str:\n",
    "\n",
    "        few_shot_prompt = \"\"\"\n",
    "        You are a rating prediction system. Learn from these examples:\n",
    "\n",
    "        Example 1 - 5 stars:\n",
    "        Review: \"Absolutely amazing experience! The food was outstanding, service was impeccable, and the atmosphere was perfect. Best restaurant in town!\"\n",
    "        Rating: {{\"predicted_stars\": 5, \"explanation\": \"Extremely positive language with multiple superlatives and no complaints\"}}\n",
    "\n",
    "        Example 2 - 4 stars:\n",
    "        Review: \"Really good food and nice staff. The wait was a bit long but overall a great experience. Would definitely come back.\"\n",
    "        Rating: {{\"predicted_stars\": 4, \"explanation\": \"Positive overall with one minor negative aspect mentioned\"}}\n",
    "\n",
    "        Example 3 - 3 stars:\n",
    "        Review: \"The food was decent but nothing special. Service was okay. It's fine for a quick meal but I wouldn't go out of my way to come here.\"\n",
    "        Rating: {{\"predicted_stars\": 3, \"explanation\": \"Neutral language with mixed sentiments, neither strongly positive nor negative\"}}\n",
    "\n",
    "        Example 4 - 2 stars:\n",
    "        Review: \"Pretty disappointed. The food was cold and the service was slow. A few items were good but mostly not worth the price.\"\n",
    "        Rating: {{\"predicted_stars\": 2, \"explanation\": \"Predominantly negative with slight positive mention, expressing disappointment\"}}\n",
    "\n",
    "        Example 5 - 1 star:\n",
    "        Review: \"Horrible experience. Rude staff, terrible food, dirty environment. Complete waste of money. Never coming back!\"\n",
    "        Rating: {{\"predicted_stars\": 1, \"explanation\": \"Extremely negative with multiple serious complaints and no positive aspects\"}}\n",
    "\n",
    "        Now analyze this review:\n",
    "        Review: \"{review_text}\"\n",
    "\n",
    "        Return ONLY a JSON object in the exact same format (no markdown, no extra text):\n",
    "        {{\"predicted_stars\": <1-5>, \"explanation\": \"<brief reasoning>\"}}\n",
    "        \"\"\"\n",
    "\n",
    "        return few_shot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa8c7a0",
   "metadata": {},
   "source": [
    "### Approach 3 : Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChainOfThoughtApproach(PromptApproach):\n",
    "        \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name = \"Chain of Thought (CoT)\",\n",
    "            description = \"Step-by-step reasoning before final prediction\"\n",
    "        )\n",
    "    \n",
    "    def create_prompt(self, review_text: str) -> str:\n",
    "        cot_prompt = \"\"\"\n",
    "        You are an expert at analyzing Yelp reviews. Use step-by-step reasoning to predict the star rating.\n",
    "\n",
    "        Review to analyze: \"{review_text}\"\n",
    "\n",
    "        Follow these steps:\n",
    "        1. Identify all POSITIVE aspects mentioned (quality, service, atmosphere, value, etc.)\n",
    "        2. Identify all NEGATIVE aspects or complaints\n",
    "        3. Assess the overall sentiment intensity (mild, moderate, strong, extreme)\n",
    "        4. Consider the language used (neutral, emotional, superlatives, etc.)\n",
    "        5. Determine if there are any deal-breakers or exceptional highlights\n",
    "\n",
    "        Based on your analysis:\n",
    "        - 5 stars: Overwhelmingly positive, exceptional experience\n",
    "        - 4 stars: Very positive, minor issues don't overshadow the good\n",
    "        - 3 stars: Balanced or neutral, significant pros and cons\n",
    "        - 2 stars: Predominantly negative, few redeeming qualities  \n",
    "        - 1 star: Extremely negative, multiple serious problems\n",
    "\n",
    "        After analyzing, provide your rating in this EXACT JSON format (no markdown):\n",
    "        {{\n",
    "          \"predicted_stars\": <number between 1-5>,\n",
    "          \"explanation\": \"<2-3 sentence explanation of your reasoning>\"\n",
    "        }}\n",
    "        \"\"\"\n",
    "        \n",
    "        return cot_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a0ce3",
   "metadata": {},
   "source": [
    "## 6. EVALUATION FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39fc3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_approach(approach: PromptApproach, df: pd.DataFrame, sample_size: int = None) -> Dict:\n",
    "\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üîÑ Evaluating: {approach.name}\")\n",
    "    print(f\"Description: {approach.description}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Use subset if specified\n",
    "    eval_df = df.sample(n=sample_size, random_state=42) if sample_size else df\n",
    "    \n",
    "    predictions = []\n",
    "    valid_json_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, row in eval_df.iterrows():\n",
    "        review_text = str(row['text'])  # Ensure it's a string\n",
    "        actual_rating = int(row['stars'])  # Ensure it's an int\n",
    "        \n",
    "        print(f\"\\nüìù Processing review {idx + 1}/{len(eval_df)}...\")\n",
    "        print(f\"Review preview: {review_text[:100]}...\")\n",
    "        \n",
    "        # Make prediction\n",
    "        try:\n",
    "            result = approach.predict(review_text)\n",
    "            print(f\"‚úÖ Prediction: {result['predicted_stars']} stars\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during prediction: {str(e)}\")\n",
    "            result = {\n",
    "                'predicted_stars': None,\n",
    "                'explanation': f\"Prediction error: {str(e)}\",\n",
    "                'valid_json': False\n",
    "            }\n",
    "        \n",
    "        predictions.append({\n",
    "            'actual_stars': actual_rating,\n",
    "            'predicted_stars': result['predicted_stars'],\n",
    "            'explanation': result['explanation'],\n",
    "            'review_text': review_text[:100] + '...',\n",
    "            'valid_json': result['valid_json']\n",
    "        })\n",
    "        \n",
    "        if result['valid_json']:\n",
    "            valid_json_count += 1\n",
    "        \n",
    "        # Rate limiting - be nice to the API\n",
    "        time.sleep(1.0)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    \n",
    "    # Filter out invalid predictions for accuracy calculation\n",
    "    valid_preds = pred_df[pred_df['valid_json'] == True].copy()\n",
    "    \n",
    "    if len(valid_preds) == 0:\n",
    "        print(\"‚ùå No valid predictions generated!\")\n",
    "        return {\n",
    "            'approach_name': approach.name,\n",
    "            'predictions': pred_df,\n",
    "            'metrics': {}\n",
    "        }\n",
    "    \n",
    "    accuracy = accuracy_score(valid_preds['actual_stars'], valid_preds['predicted_stars'])\n",
    "    mae = mean_absolute_error(valid_preds['actual_stars'], valid_preds['predicted_stars'])\n",
    "    json_validity_rate = (valid_json_count / len(predictions)) * 100\n",
    "    avg_time_per_prediction = elapsed_time / len(predictions)\n",
    "    \n",
    "    # Calculate per-rating accuracy\n",
    "    per_rating_accuracy = {}\n",
    "    for rating in range(1, 6):\n",
    "        rating_preds = valid_preds[valid_preds['actual_stars'] == rating]\n",
    "        if len(rating_preds) > 0:\n",
    "            rating_acc = (rating_preds['actual_stars'] == rating_preds['predicted_stars']).mean()\n",
    "            per_rating_accuracy[rating] = rating_acc\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'mae': mae,\n",
    "        'json_validity_rate': json_validity_rate,\n",
    "        'avg_time_per_prediction': avg_time_per_prediction,\n",
    "        'total_predictions': len(predictions),\n",
    "        'valid_predictions': len(valid_preds),\n",
    "        'per_rating_accuracy': per_rating_accuracy\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nüìä Results for {approach.name}:\")\n",
    "    print(f\"  Accuracy: {accuracy:.2%}\")\n",
    "    print(f\"  MAE: {mae:.3f} stars\")\n",
    "    print(f\"  JSON Validity Rate: {json_validity_rate:.1f}%\")\n",
    "    print(f\"  Avg Time per Prediction: {avg_time_per_prediction:.2f}s\")\n",
    "    print(f\"  Total Time: {elapsed_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'approach_name': approach.name,\n",
    "        'predictions': pred_df,\n",
    "        'metrics': metrics\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c142e7",
   "metadata": {},
   "source": [
    "## 7. RUN ALL EVALUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d800672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize approaches\n",
    "approaches = [\n",
    "    ZeroShotApproach(),\n",
    "    FewShotApproach(),\n",
    "    ChainOfThoughtApproach()\n",
    "]\n",
    "\n",
    "# Evaluate all approaches\n",
    "# Note: Using 200 reviews as recommended, change to None to use all 250\n",
    "results = []\n",
    "\n",
    "for approach in approaches:\n",
    "    result = evaluate_approach(approach, df, sample_size=200)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Save predictions to JSON\n",
    "    filename = f\"results/predictions_{approach.name.lower().replace(' ', '_').replace('-', '_')}.json\"\n",
    "    result['predictions'].to_json(filename, orient='records', indent=2)\n",
    "    print(f\"üíæ Saved predictions to {filename}\")\n",
    "\n",
    "print(\"\\n‚úÖ All evaluations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a222f42",
   "metadata": {},
   "source": [
    "## 8. COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9365c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_table(results: List[Dict]) -> pd.DataFrame:\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for result in results:\n",
    "        metrics = result['metrics']\n",
    "        if not metrics:\n",
    "            continue\n",
    "            \n",
    "        comparison_data.append({\n",
    "            'Approach': result['approach_name'],\n",
    "            'Model': SELECTED_MODEL.split('/')[-1],\n",
    "            'Accuracy (%)': f\"{metrics['accuracy']*100:.2f}\",\n",
    "            'MAE': f\"{metrics['mae']:.3f}\",\n",
    "            'JSON Validity (%)': f\"{metrics['json_validity_rate']:.1f}\",\n",
    "            'Avg Time (s)': f\"{metrics['avg_time_per_prediction']:.2f}\",\n",
    "            'Valid Predictions': metrics['valid_predictions']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    return comparison_df\n",
    "\n",
    "comparison_table = create_comparison_table(results)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä COMPARISON TABLE\")\n",
    "print(\"=\"*80)\n",
    "print(comparison_table.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save comparison table\n",
    "comparison_table.to_csv('results/comparison_results_openrouter.csv', index=False)\n",
    "print(\"\\nüíæ Saved comparison table to results/comparison_results_openrouter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac355d",
   "metadata": {},
   "source": [
    "## 9. VISUALIZATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaced4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if any(r['metrics'] for r in results):\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'Yelp Rating Prediction - OpenRouter ({SELECTED_MODEL.split(\"/\")[-1]})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Plot 1: Accuracy Comparison\n",
    "    ax = axes[0, 0]\n",
    "    accuracies = [r['metrics']['accuracy']*100 for r in results if r['metrics']]\n",
    "    approach_names = [r['approach_name'] for r in results if r['metrics']]\n",
    "    bars = ax.bar(approach_names, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax.set_ylabel('Accuracy (%)', fontweight='bold')\n",
    "    ax.set_title('Accuracy Comparison', fontweight='bold')\n",
    "    ax.set_ylim([0, 100])\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    # Plot 2: MAE Comparison\n",
    "    ax = axes[0, 1]\n",
    "    maes = [r['metrics']['mae'] for r in results if r['metrics']]\n",
    "    bars = ax.bar(approach_names, maes, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax.set_ylabel('Mean Absolute Error', fontweight='bold')\n",
    "    ax.set_title('MAE Comparison (Lower is Better)', fontweight='bold')\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    # Plot 3: JSON Validity Rate\n",
    "    ax = axes[0, 2]\n",
    "    validity_rates = [r['metrics']['json_validity_rate'] for r in results if r['metrics']]\n",
    "    bars = ax.bar(approach_names, validity_rates, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    ax.set_ylabel('JSON Validity Rate (%)', fontweight='bold')\n",
    "    ax.set_title('JSON Validity Rate', fontweight='bold')\n",
    "    ax.set_ylim([0, 100])\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "    # Plot 4-6: Confusion Matrices\n",
    "    for idx, result in enumerate(results):\n",
    "        if not result['metrics']:\n",
    "            continue\n",
    "        \n",
    "        ax = axes[1, idx]\n",
    "        valid_preds = result['predictions'][result['predictions']['valid_json'] == True]\n",
    "        \n",
    "        if len(valid_preds) > 0:\n",
    "            cm = confusion_matrix(valid_preds['actual_stars'], valid_preds['predicted_stars'], \n",
    "                                 labels=[1, 2, 3, 4, 5])\n",
    "            \n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, \n",
    "                        xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "            ax.set_xlabel('Predicted Stars', fontweight='bold')\n",
    "            ax.set_ylabel('Actual Stars', fontweight='bold')\n",
    "            ax.set_title(f'{result[\"approach_name\"]} - Confusion Matrix', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/comparison_visualizations_openrouter.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"üíæ Saved visualizations to results/comparison_visualizations_openrouter.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid results to visualize\")\n",
    "\n",
    "print(\"\\n‚úÖ Analysis complete! Check the results/ folder for all outputs.\")\n",
    "print(f\"\\nüí° Tip: Try experimenting with different models by changing SELECTED_MODEL at the top!\")\n",
    "print(f\"   Current model: {SELECTED_MODEL}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fynd_env (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
